import { Post } from "../../types/post";

export const post: Post = {
  title: "The Hackneyed Brain: 7 Cognitive Biases Sabotaging Your Decisions",
  excerpt: "You are not as rational as you think. From Confirmation Bias to the Sunk Cost Fallacy, discover the invisible scripts running your life and how to rewrite them.",
  category: "psychology",
  date: "January 28, 2026",
  slug: "cognitive-biases-judgment",
  imageUrl: "https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?q=80&w=1000&auto=format&fit=crop",
  content: `
    <article>
        <p>
            We like to think of ourselves as rational agents—piloting our lives with logic, weighing evidence objectively, and making decisions based on facts. The truth is much more humbling. Our brains are not designed for truth; they are designed for survival and efficiency.
        </p>
        <p>
            To handle the millions of bits of information bombing us every day, the brain uses "Heuristics"—mental shortcuts. While these shortcuts kept our ancestors safe from saber-toothed tigers, in the modern world, they often misfire. These misfires are called <strong>Cognitive Biases</strong>. They are the systemic errors in our thinking that affect our judgments and decisions. Here are the most powerful ones you need to know.
        </p>

        <h2>1. Confirmation Bias: The Filter Bubble</h2>
        <p>
            This is the king of all biases. Confirmation bias is our tendency to search for, interpret, and recall information in a way that confirms what we already believe.
        </p>
        <p>
            <em>Example:</em> If you believe "Site A" is fake news, you will critically analyze every typo in their articles. If you believe "Site B" is trustworthy, you will overlook their mistakes as "honest errors." In the age of algorithms, this bias is weaponized. Social media feeds show us what we want to see, creating "Echo Chambers" where we become more and more radicalized because we never see opposing evidence.
            <br>
            <strong>The Fix:</strong> Actively seek out information that disproves your theory. Ask, "What would I need to see to change my mind?"
        </p>

        <h2>2. The Sunk Cost Fallacy: Throwing Good Money After Bad</h2>
        <p>
            This bias tells us to continue an endeavor just because we’ve already invested time, money, or effort into it, even if the current costs outweigh the benefits.
        </p>
        <p>
            <em>Example:</em> You are watching a terrible movie. You’ve paid $15 for the ticket and watched 30 minutes. You stay for the remaining 90 minutes solely because "I paid for it." Rationally, the money is gone. Staying just wastes your time too. It keeps people in bad jobs, bad relationships, and bad investments.
            <br>
            <strong>The Fix:</strong> ask yourself: "If I hadn't invested anything yet, would I start this today?"
        </p>

        <h2>3. The Anchoring Effect: The Power of the First Impression</h2>
        <p>
            We rely too heavily on the first piece of information (the "anchor") we receive when making decisions.
        </p>
        <p>
            <em>Example:</em> You walk into a luxury store and see a bag for $5,000. Startled, you turn away. Then you see a bag for $1,200. Suddenly, it seems "cheap." It isn't cheap; it's $1,200. But compared to the $5,000 anchor, your brain perceives it as a bargain. Salary negotiations work the same way; the first number spoken sets the range for the entire conversation.
        </p>

        <h2>4. The Availability Heuristic: Why We Fear Sharks but Not Cars</h2>
        <p>
            We judge the likelihood of an event based on how easily examples come to mind. If we can recall it vividly, we think it’s common.
        </p>
        <p>
            <em>Example:</em> After a plane crash is on the news, people are terrified to fly. Yet, the drive to the airport is statistically hundreds of times more dangerous. Plane crashes are dramatic and memorable (High Availability); car crashes are mundane and forgotten (Low Availability). This bias causes us to misallocate our worry and resources, focusing on rare terrors instead of probable risks like heart disease.
        </p>

        <h2>5. The Dunning-Kruger Effect: The Confidence of Ignorance</h2>
        <p>
            This is a cognitive bias where people with low ability at a task overestimate their ability. Essentially, they don't know enough to know how much they don't know.
        </p>
        <p>
            <em>Example:</em> The amateur who reads two articles on nutrition and starts lecturing a dietitian. Conversely, experts often suffer from "Imposter Syndrome"—underestimating their competence because they assume what is easy for them is easy for everyone.
            <br>
            <strong>The Fix:</strong> "Real knowledge is to know the extent of one's ignorance." - Confucius.
        </p>

        <h2>6. Negativity Bias: Velcro for Bad, Teflon for Good</h2>
        <p>
            Our brains are wired to pay more attention to negative news than positive news. Evolutionarily, noticing a lion was more important than noticing a pretty flower.
        </p>
        <p>
            <em>Example:</em> You can get 10 compliments and 1 insult. You will forget the compliments by lunch, but you will think about the insult for a week. This bias explains why the news is always negative—it grabs clear attention.
            <br>
            <strong>The Fix:</strong> Keep a "Gratitude Journal." You must manually force your brain to encode positive experiences, or they will slip away.
        </p>

        <h2>7. Fundamental Attribution Error</h2>
        <p>
            We judge others by their actions, but ourselves by our intentions.
        </p>
        <p>
            <em>Example:</em> If someone cuts you off in traffic, you think, "They are a jerk." If you cut someone off, you think, "I'm in a rush because I'm late for a meeting." We attribute other people's mistakes to their character (internal), but our mistakes to the situation (external).
        </p>

        <h2>Conclusion: Becoming Less Wrong</h2>
        <p>
            You cannot delete these biases; they are part of your operating system. But you can patch the software. By practicing <strong>Metacognition</strong>—thinking about your thinking—you can catch these glitches in real-time.
        </p>
        <p>
            Slowing down, seeking outside perspectives, and remaining humble about your own rationality are the only defenses we have. In a complex world, the most dangerous lies are the ones we tell ourselves.
        </p>
    </article>
    `
};
